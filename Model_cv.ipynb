{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f502ed-c312-42e0-8b38-713db0dc060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "from pyannote.audio import Inference\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face token from environment\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56581ce3-17d8-4020-9336-e0bd343e7fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e3811-5181-4650-b8a3-b2dd6803f4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WAGHMARE\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\WAGHMARE\\.cache\\torch\\pyannote\\models--pyannote--embedding\\snapshots\\4db4899737a38b2d618bbd74350915aa10293cb2\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WAGHMARE\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['loss_func.W']\n"
     ]
    }
   ],
   "source": [
    "inference = Inference(\"pyannote/embedding\",\n",
    "                      use_auth_token=HF_TOKEN,\n",
    "                      window=\"whole\",\n",
    "                      device=device)\n",
    "MAX_DURATION = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b22429e-2f06-41e6-8215-031e3c19c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_dir(directory):\n",
    "    wav_files = glob.glob(os.path.join(directory, \"**\", \"*.wav\"), recursive=True)\n",
    "    print(f\"\\n Found {len(wav_files)} audio files in {directory}\")\n",
    "\n",
    "    embeddings = []\n",
    "    file_labels = []\n",
    "\n",
    "    for path in tqdm(wav_files, desc=f\"Processing {os.path.basename(directory)}\"):\n",
    "        try:\n",
    "            waveform, sr = librosa.load(path, sr=16000, mono=True)\n",
    "            if len(waveform) > MAX_DURATION * sr:\n",
    "                waveform = waveform[:int(MAX_DURATION * sr)]\n",
    "            else:\n",
    "                pad_len = int(MAX_DURATION * sr) - len(waveform)\n",
    "                waveform = np.pad(waveform, (0, pad_len), 'constant')\n",
    "\n",
    "            inputs = {\n",
    "                \"waveform\": torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).to(device),\n",
    "                \"sample_rate\": sr\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedding = inference(inputs)\n",
    "\n",
    "            embeddings.append(embedding.flatten())\n",
    "            file_labels.append(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "    return embeddings, file_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d86bb1-b78f-4078-a8aa-903f0cb45362",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_DIR = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_en\"\n",
    "HINDI_DIR = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968a34b0-5ebb-4fd6-be66-910670cfa638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Found 125 audio files in C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Processing processed_en: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:22<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_en, labels_en = extract_embeddings_from_dir(ENGLISH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9457cfc-f5bc-40cd-9839-1f539938e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Found 1891 audio files in C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Processing processed_hi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1891/1891 [07:21<00:00,  4.28it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_hi, labels_hi = extract_embeddings_from_dir(HINDI_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75f3a0cc-d26c-47dd-837c-99fca1f7f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embedding(embedding):\n",
    "    return (embedding - np.mean(embedding)) / np.std(embedding)\n",
    "\n",
    "def compare_audio_files(file1, file2, inference_model, device, duration=5.0):\n",
    "    def process_audio(path):\n",
    "        \n",
    "        waveform, sr = librosa.load(path, sr=16000, mono=True)\n",
    "\n",
    "        \n",
    "        if len(waveform) > duration * sr:\n",
    "            waveform = waveform[:int(duration * sr)]\n",
    "        else:\n",
    "            pad_length = int(duration * sr) - len(waveform)\n",
    "            waveform = np.pad(waveform, (0, pad_length), 'constant')\n",
    "\n",
    "        \n",
    "        inputs = {\n",
    "            \"waveform\": torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).to(device),\n",
    "            \"sample_rate\": sr\n",
    "        }\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = inference_model(inputs)\n",
    "\n",
    "            \n",
    "            if isinstance(embedding, dict):\n",
    "                embedding = embedding[\"embedding\"]\n",
    "\n",
    "            \n",
    "            if isinstance(embedding, torch.Tensor):\n",
    "                embedding = embedding.detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "            embedding = embedding.flatten()\n",
    "\n",
    "        return embedding\n",
    "\n",
    "    \n",
    "    emb1 = process_audio(file1)\n",
    "    emb2 = process_audio(file2)\n",
    "\n",
    "    \n",
    "    emb1_normalized = normalize_embedding(emb1)\n",
    "    emb2_normalized = normalize_embedding(emb2)\n",
    "\n",
    "    \n",
    "    print(f\"Embedding 1 (First 10 values): {emb1_normalized[:10]}\")\n",
    "    print(f\"Embedding 2 (First 10 values): {emb2_normalized[:10]}\")\n",
    "\n",
    "    \n",
    "    similarity = cosine_similarity([emb1_normalized], [emb2_normalized])[0][0]\n",
    "    \n",
    "    # Print the similarity score\n",
    "    print(f\"\\nüéôÔ∏è Similarity between:\\n'{file1}'\\nand\\n'{file2}' ‚ûú {similarity:.4f}\")\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f97706c-b0fb-42df-bf3c-ad5d0b05ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 (First 10 values): [ 2.5484045   0.09778892  1.5351144   0.78593045  0.98404956  1.7880502\n",
      " -0.27377176  0.2508088  -0.2613017  -0.01528185]\n",
      "Embedding 2 (First 10 values): [ 1.544618   -1.5865766  -0.23792142 -0.81557417  0.55871034  0.6480172\n",
      " -0.7324481   1.0108807  -1.2180551  -0.31522748]\n",
      "\n",
      "üéôÔ∏è Similarity between:\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\0e97e303991e6646c845049579a4bea7a25ea775f6b8b44a09e0efd0ae9661ec984c393c92d2522e3158c8b0ca206ad60fd9e136713e18c21dffd72811a88b88\\common_voice_hi_38108255.wav'\n",
      "and\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\0e97e303991e6646c845049579a4bea7a25ea775f6b8b44a09e0efd0ae9661ec984c393c92d2522e3158c8b0ca206ad60fd9e136713e18c21dffd72811a88b88\\common_voice_hi_38112196.wav' ‚ûú 0.5653\n"
     ]
    }
   ],
   "source": [
    "file_path_1 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\0e97e303991e6646c845049579a4bea7a25ea775f6b8b44a09e0efd0ae9661ec984c393c92d2522e3158c8b0ca206ad60fd9e136713e18c21dffd72811a88b88\\common_voice_hi_38108255.wav\"\n",
    "file_path_2 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\0e97e303991e6646c845049579a4bea7a25ea775f6b8b44a09e0efd0ae9661ec984c393c92d2522e3158c8b0ca206ad60fd9e136713e18c21dffd72811a88b88\\common_voice_hi_38112196.wav\"\n",
    "similarity_score = compare_audio_files(file_path_1, file_path_2, inference, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa93720b-9dfc-4fc9-b2ce-179415ed4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 (First 10 values): [ 3.177615   -2.0175655  -0.9942628  -0.35604155 -0.09130063 -0.10804775\n",
      " -0.31795043  0.46639934 -0.37669405  0.7474587 ]\n",
      "Embedding 2 (First 10 values): [ 0.21342622 -0.19962388  1.0655699  -0.18776372 -1.0166669   0.7234538\n",
      " -1.234155   -0.8617688  -2.2636068   0.10148074]\n",
      "\n",
      "üéôÔ∏è Similarity between:\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A07_F\\en_A07_F_01.wav'\n",
      "and\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A07_F\\en_A07_F_04.wav' ‚ûú 0.2848\n"
     ]
    }
   ],
   "source": [
    "file_path_1 = r\"C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A07_F\\en_A07_F_01.wav\"\n",
    "file_path_2 = r\"C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A07_F\\en_A07_F_04.wav\"\n",
    "similarity_score = compare_audio_files(file_path_1, file_path_2, inference, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a695c5c3-e344-4a7a-9fb5-7b0970a5755b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 (First 10 values): [ 0.06538545 -1.8003957  -0.41515714  1.1346269   1.5001671  -0.87405515\n",
      " -0.7823379   0.04286908 -1.2332816  -0.5366544 ]\n",
      "Embedding 2 (First 10 values): [ 0.7052868  -2.4616885  -0.05391465  0.8576792   1.5719087  -1.9978913\n",
      "  0.23450336  0.3896977  -0.926255   -0.9599938 ]\n",
      "\n",
      "üéôÔ∏è Similarity between:\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker_0001\\Speaker_0001_00002.wav'\n",
      "and\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker_0001\\Speaker_0001_00004.wav' ‚ûú 0.8249\n"
     ]
    }
   ],
   "source": [
    "file_path_1 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker_0001\\Speaker_0001_00002.wav\"\n",
    "file_path_2 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker_0001\\Speaker_0001_00004.wav\"\n",
    "similarity_score = compare_audio_files(file_path_1, file_path_2, inference, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e309751-5c77-44ae-af67-2741f93e432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 (First 10 values): [ 1.8196362e-01 -2.2389858e+00  1.5574834e+00  1.1474992e-01\n",
      " -6.8704617e-01  8.9194947e-01  2.2660670e-01  1.0181997e-03\n",
      "  9.2620069e-01  4.6025634e-01]\n",
      "Embedding 2 (First 10 values): [ 0.47011063 -0.4088314  -0.17669976  1.3997208   0.48099512 -0.3109015\n",
      " -0.45904306 -1.2318515  -2.5297658   0.3463721 ]\n",
      "\n",
      "üéôÔ∏è Similarity between:\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker0050\\Speaker0050_002.wav'\n",
      "and\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker0039\\Speaker0039_001.wav' ‚ûú -0.0407\n"
     ]
    }
   ],
   "source": [
    "file_path_1 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker0050\\Speaker0050_002.wav\"\n",
    "file_path_2 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\50_speakers_audio_data\\Speaker0039\\Speaker0039_001.wav\"\n",
    "similarity_score = compare_audio_files(file_path_1, file_path_2, inference, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a3d83e-96df-43b1-9590-e7d8f23922cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 (First 10 values): [ 2.1595752  -0.30694318  1.4633367   1.8988129   0.76514125 -1.6562616\n",
      " -1.2709295   0.38310316 -0.6176218   0.40029523]\n",
      "Embedding 2 (First 10 values): [ 1.7465259   0.2329593  -1.184087    0.40510708  0.76387626  0.44100273\n",
      " -1.0300514   0.61136824  0.07249195 -0.7768629 ]\n",
      "\n",
      "üéôÔ∏è Similarity between:\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\1ab26689a68e6aeb9ee56daf140f5d65c19295d6b65eaef9f51f22cbd867b635758194eb9640790ea7d37a0de2bd5d0f235fb7662cc80dda56261f92c3cb4475\\common_voice_hi_25037973.wav'\n",
      "and\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\0a073b2441b86a13d80728f1a14413d473c4091d1053a3a53982f2c2698abb2a31f35efaefaad9b678d4cf8b27aecc058d4f664d005474a90b8af287ff58c303\\common_voice_hi_23849286.wav' ‚ûú 0.2714\n"
     ]
    }
   ],
   "source": [
    "file_path_1 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\1ab26689a68e6aeb9ee56daf140f5d65c19295d6b65eaef9f51f22cbd867b635758194eb9640790ea7d37a0de2bd5d0f235fb7662cc80dda56261f92c3cb4475\\common_voice_hi_25037973.wav\"\n",
    "file_path_2 = r\"C:\\Users\\WAGHMARE\\Desktop\\Research Project\\processed_hi\\0a073b2441b86a13d80728f1a14413d473c4091d1053a3a53982f2c2698abb2a31f35efaefaad9b678d4cf8b27aecc058d4f664d005474a90b8af287ff58c303\\common_voice_hi_23849286.wav\"\n",
    "similarity_score = compare_audio_files(file_path_1, file_path_2, inference, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5af81ffe-c559-44ba-afed-0eb9d4be4edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 (First 10 values): [ 1.7303016  -0.92037207  0.07880109 -0.01700118  0.23779546  1.046638\n",
      " -2.1089869   1.0774442   0.7817927   1.2765217 ]\n",
      "Embedding 2 (First 10 values): [ 0.61642873 -1.1677047   1.7057757   0.4080145  -0.8218774   0.6297684\n",
      "  0.16025929 -0.540241   -0.6675733  -0.78814995]\n",
      "\n",
      "üéôÔ∏è Similarity between:\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A01_M\\en_A01_M_02.wav'\n",
      "and\n",
      "'C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A08_F\\en_A08_F_05.wav' ‚ûú 0.0462\n"
     ]
    }
   ],
   "source": [
    "file_path_1 = r\"C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A01_M\\en_A01_M_02.wav\"\n",
    "file_path_2 = r\"C:\\Users\\WAGHMARE\\Desktop\\RP\\data\\English\\A08_F\\en_A08_F_05.wav\"\n",
    "similarity_score = compare_audio_files(file_path_1, file_path_2, inference, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d2657-e6bf-4b5c-9ee6-451463927968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
